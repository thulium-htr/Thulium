# HTR Model Configuration: CNN + Transformer + CTC (Large)
#
# High-capacity Transformer model for state-of-the-art accuracy.
# Designed for challenging handwriting with complex layouts.
#
# Architecture: ResNet50 -> Transformer (deep) -> CTC Decoder
#
# Specifications:
#   - Parameters: ~25M
#   - Inference: GPU required
#   - Memory: ~800MB
#
# Suitable for:
#   - Maximum accuracy requirements
#   - Historical/degraded documents
#   - Complex scripts (Arabic, CJK)
#   - Research benchmarking

model:
  name: "htr_cnn_transformer_ctc_large"
  version: "1.2.0"
  capacity: "large"
  
  backbone:
    type: "resnet"
    config_name: "resnet50"
    in_channels: 3
    output_channels: 512
    dropout: 0.1
    
  sequence_head:
    type: "transformer"
    hidden_size: 384
    num_heads: 8
    num_layers: 6
    dim_feedforward: 1536
    dropout: 0.15
    pos_encoding: "rotary"  # RoPE for better length generalization
    pre_norm: true
    
  decoder:
    type: "ctc"
    blank_index: 0
    dropout: 0.15
    
  height_projection:
    type: "adaptive_pool"
    
preprocessing:
  image_height: 64
  normalize: true
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  
training:
  optimizer:
    type: "adamw"
    lr: 0.0002
    weight_decay: 0.08
    betas: [0.9, 0.98]
    
  scheduler:
    type: "cosine"
    warmup_epochs: 15
    min_lr: 0.0000005
    
  loss:
    type: "ctc"
    blank: 0
    reduction: "mean"
    zero_infinity: true
    
  regularization:
    label_smoothing: 0.15
    dropout: 0.15
    stochastic_depth: 0.15
    layer_drop: 0.1
    
  augmentation:
    enabled: true
    random_affine:
      degrees: 6
      translate: [0.04, 0.04]
      scale: [0.85, 1.15]
    elastic_distortion:
      alpha: 40
      sigma: 7
      probability: 0.5
    noise:
      gaussian_std: 0.025
      probability: 0.35
    color_jitter:
      brightness: 0.25
      contrast: 0.25
      saturation: 0.1
      probability: 0.4
    random_erasing:
      probability: 0.2
      scale: [0.02, 0.1]
      
  batch_size: 32
  epochs: 200
  mixed_precision: true
  max_grad_norm: 0.5
  gradient_accumulation_steps: 2
  
decoding:
  method: "beam_search"
  beam_width: 30
  lm_alpha: 0.6
  lm_beta: 0.15
  length_normalization: true
  top_k: 50

inference:
  onnx_export: true
  quantization: "none"
