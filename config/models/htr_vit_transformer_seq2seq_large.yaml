# HTR Model Configuration: ViT + Transformer + Seq2Seq (Large)
#
# State-of-the-art Vision Transformer model for maximum accuracy.
# Full-capacity ViT with deep decoder for complex scripts.
#
# Architecture: ViT-Base -> Deep Transformer Decoder -> Seq2Seq
#
# Specifications:
#   - Parameters: ~45M
#   - Inference: GPU required
#   - Memory: ~1.5GB
#
# Suitable for:
#   - Maximum accuracy requirements
#   - Complex scripts (Arabic, CJK, Indic)
#   - Historical document analysis
#   - Research benchmarking and competitions

model:
  name: "htr_vit_transformer_seq2seq_large"
  version: "1.2.0"
  capacity: "large"
  architecture: "transformer"
  
  backbone:
    type: "vit"
    image_size: [32, 512]
    patch_size: 16
    dim: 768
    depth: 12
    heads: 12
    mlp_dim: 3072
    dropout: 0.1
    pretrained: true
    pretrained_weights: "imagenet21k"
    
  head:
    type: "transformer_encoder"
    d_model: 768
    nhead: 8
    num_layers: 6
    dim_feedforward: 2048
    dropout: 0.1
    pre_norm: true
    
  decoder:
    type: "transformer_decoder"
    d_model: 768
    nhead: 8
    num_layers: 6
    num_classes: 100
    dropout: 0.1
    max_len: 128
    pre_norm: true
    tie_embeddings: true
    
preprocessing:
  image_height: 32
  image_width: 512
  normalize: true
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

training:
  optimizer:
    type: "adamw"
    lr: 0.0001
    weight_decay: 0.08
    betas: [0.9, 0.98]
    
  scheduler:
    type: "cosine"
    warmup_epochs: 20
    min_lr: 0.0000005
    
  loss:
    type: "cross_entropy"
    label_smoothing: 0.15
    
  regularization:
    dropout: 0.1
    stochastic_depth: 0.2
    layer_drop: 0.1
    attention_dropout: 0.1
    
  augmentation:
    enabled: true
    random_affine:
      degrees: 6
      translate: [0.04, 0.04]
      scale: [0.85, 1.15]
    elastic_distortion:
      alpha: 40
      sigma: 7
      probability: 0.5
    color_jitter:
      brightness: 0.25
      contrast: 0.25
      saturation: 0.1
      probability: 0.4
    mixup:
      alpha: 0.3
      probability: 0.4
    cutout:
      num_holes: 2
      max_h_size: 16
      max_w_size: 32
      probability: 0.3
      
  batch_size: 24
  epochs: 250
  mixed_precision: true
  max_grad_norm: 0.5
  gradient_accumulation_steps: 4
  
  # EMA for stable training
  ema:
    enabled: true
    decay: 0.9999

decoding:
  method: "beam_search"
  beam_width: 40
  length_penalty: 1.2
  max_length: 128
  diversity_penalty: 0.0
  early_stopping: true

inference:
  onnx_export: true
  quantization: "none"
  batch_size: 8
