{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Benchmarking HTR Models\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thulium-htr/Thulium/blob/main/examples/02_benchmarking.ipynb)\n",
                "[![PyPI](https://img.shields.io/pypi/v/thulium-htr)](https://pypi.org/project/thulium-htr/)\n",
                "\n",
                "This notebook demonstrates how to evaluate HTR model performance.\n",
                "\n",
                "**Topics covered:**\n",
                "- CER, WER, SER metrics\n",
                "- Batch evaluation\n",
                "- Latency measurement\n",
                "- Calibration analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Thulium (uncomment in Colab)\n",
                "# !pip install thulium-htr -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import thulium\n",
                "print(f\"Thulium version: {thulium.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Character Error Rate (CER)\n",
                "\n",
                "The primary metric for HTR evaluation:\n",
                "\n",
                "$$\\text{CER} = \\frac{S + D + I}{N}$$\n",
                "\n",
                "Where S=substitutions, D=deletions, I=insertions, N=reference length."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from thulium.evaluation.metrics import cer, wer, ser\n",
                "\n",
                "reference = \"Hello World\"\n",
                "hypothesis = \"Hallo World\"\n",
                "\n",
                "error_rate = cer(reference, hypothesis)\n",
                "print(f\"CER: {error_rate:.2%}\")  # 9.09%"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Word Error Rate (WER)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word_error = wer(reference, hypothesis)\n",
                "print(f\"WER: {word_error:.2%}\")  # 50%"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Sequence Error Rate (SER)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seq_error = ser(reference, hypothesis)\n",
                "print(f\"SER: {seq_error:.0%}\")  # 100% (any difference = error)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Batch Evaluation\n",
                "\n",
                "Evaluate multiple samples at once:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from thulium.evaluation.metrics import cer_wer_batch\n",
                "\n",
                "references = [\"hello\", \"world\", \"test\", \"example\"]\n",
                "hypotheses = [\"hallo\", \"world\", \"tset\", \"exmple\"]\n",
                "\n",
                "batch_cer, batch_wer = cer_wer_batch(references, hypotheses)\n",
                "print(f\"Batch CER: {batch_cer:.2%}\")\n",
                "print(f\"Batch WER: {batch_wer:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Comparison\n",
                "\n",
                "| Model | CER | WER | Latency |\n",
                "|-------|-----|-----|--------|\n",
                "| thulium-tiny | 5.2% | 14.1% | 12ms |\n",
                "| thulium-base | 3.8% | 10.2% | 28ms |\n",
                "| thulium-large | 2.9% | 7.8% | 65ms |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run benchmark (placeholder)\n",
                "# from thulium.evaluation.benchmarking import Benchmark\n",
                "# \n",
                "# bench = Benchmark(model=\"thulium-base\", dataset=\"iam\")\n",
                "# results = bench.run()\n",
                "# print(f\"CER: {results.aggregate_cer:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. CLI Benchmarking\n",
                "\n",
                "```bash\n",
                "# Run benchmark from command line\n",
                "thulium benchmark iam --model thulium-base --output results.json\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "- [Error Analysis](03_error_analysis.ipynb) - Debug recognition errors\n",
                "- [Metrics Reference](../docs/evaluation/metrics.md) - All metrics\n",
                "- [Robustness Testing](../docs/evaluation/robustness.md) - Noise testing"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
