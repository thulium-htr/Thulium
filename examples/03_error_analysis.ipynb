{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Error Analysis for HTR\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/olaflaitinen/Thulium/blob/main/examples/03_error_analysis.ipynb)\n",
                "[![PyPI](https://img.shields.io/pypi/v/thulium-htr)](https://pypi.org/project/thulium-htr/)\n",
                "\n",
                "This notebook demonstrates how to analyze recognition errors.\n",
                "\n",
                "**Topics covered:**\n",
                "- Error summarization\n",
                "- Top-K error analysis\n",
                "- Character-level debugging\n",
                "- Common error patterns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Thulium (uncomment in Colab)\n",
                "# !pip install thulium-htr -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import thulium\n",
                "print(f\"Thulium version: {thulium.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Sample Data\n",
                "\n",
                "Let's create some example predictions and ground truth:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictions = [\n",
                "    \"The quick brown fox\",\n",
                "    \"jumps over the lazy dog\",\n",
                "    \"Hello World\",\n",
                "    \"Testing 123\",\n",
                "]\n",
                "\n",
                "ground_truth = [\n",
                "    \"The quick brown fox\",  # Correct\n",
                "    \"jumps over the lazy dog\",  # Correct\n",
                "    \"Hallo World\",  # Error: Hello -> Hallo\n",
                "    \"Testing 1234\",  # Error: missing 4\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Error Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from thulium.xai.error_analysis import ErrorAnalyzer\n",
                "\n",
                "# Get aggregate metrics\n",
                "metrics = ErrorAnalyzer.summarize_metrics(predictions, ground_truth)\n",
                "\n",
                "print(\"Summary Metrics:\")\n",
                "print(f\"  CER: {metrics['CER']:.2%}\")\n",
                "print(f\"  WER: {metrics['WER']:.2%}\")\n",
                "print(f\"  SER: {metrics['SER']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Top-K Errors\n",
                "\n",
                "Find the most frequent errors:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "top_errors = ErrorAnalyzer.analyze_top_k_errors(\n",
                "    predictions, \n",
                "    ground_truth, \n",
                "    k=10\n",
                ")\n",
                "\n",
                "print(\"\\nTop Errors:\")\n",
                "for i, error in enumerate(top_errors, 1):\n",
                "    print(f\"{i}. '{error['ground_truth']}' → '{error['prediction']}'\")\n",
                "    print(f\"   Count: {error['count']}, CER: {error['cer']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Common Error Patterns\n",
                "\n",
                "| Pattern | Example | Cause | Fix |\n",
                "|---------|---------|-------|-----|\n",
                "| Similar chars | o → 0, l → 1 | Font ambiguity | More training data |\n",
                "| Missing punct | . omitted | Bias in training | Augment punctuation |\n",
                "| Wrong case | A → a | Case sensitivity | Normalize case |\n",
                "| Extra spaces | word  word | Segmentation | Better preprocessing |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Character-Level Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from thulium.evaluation.metrics import cer\n",
                "\n",
                "# Analyze a specific error\n",
                "ref = \"hello\"\n",
                "hyp = \"hallo\"\n",
                "\n",
                "error = cer(ref, hyp)\n",
                "print(f\"'{ref}' → '{hyp}'\")\n",
                "print(f\"CER: {error:.2%}\")\n",
                "print(f\"Error: 'e' was substituted with 'a'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. CLI Error Analysis\n",
                "\n",
                "```bash\n",
                "# Analyze errors from command line\n",
                "thulium analyze predictions.txt ground_truth.txt \\\n",
                "    --top-k 50 \\\n",
                "    --format markdown \\\n",
                "    --output error_report.md\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "- [Attention Visualization](../docs/xai/attention_and_saliency.md) - See what the model looks at\n",
                "- [Training Guide](../docs/training/training_guide.md) - Improve model accuracy\n",
                "- [Robustness Testing](../docs/evaluation/robustness.md) - Test noise resistance"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}